{
    "g-google-sheets-importhtml": {
      "answer": "134",
      "question": "Sports Analytics for CricketPro CricketPro Insights is a leading sports analytics firm specializing in providing in-depth statistical analysis and insights for cricket teams, coaches, and enthusiasts. Leveraging data from prominent sources like ESPN Cricinfo, CricketPro offers actionable intelligence that helps teams optimize player performance, strategize game plans, and engage with fans through detailed statistics and visualizations. In the competitive world of cricket, understanding player performance metrics is crucial for team selection, game strategy, and player development. However, manually extracting and analyzing batting statistics from extensive datasets spread across multiple web pages is time-consuming and prone to errors. To maintain their edge and deliver timely insights, CricketPro needs an efficient, automated solution to aggregate and analyze player performance data from ESPN Cricinfo's ODI (One Day International) batting statistics. CricketPro Insights has identified the need to automate the extraction and analysis of ODI batting statistics from ESPN Cricinfo to streamline their data processing workflow. The statistics are available on a paginated website, with each page containing a subset of player data. By automating this process, CricketPro aims to provide up-to-date insights on player performances, such as the number of duck outs (i.e. a score of zero), which are pivotal for team assessments and strategic planning. As part of this initiative, you are tasked with developing a solution that allows CricketPro analysts to: Navigate Paginated Data: Access specific pages of the ODI batting statistics based on varying requirements. Extract Relevant Data: Use Google Sheets' IMPORTHTML function to pull tabular data from ESPN Cricinfo. Analyze Performance Metrics: Count the number of ducks (where the player was out for 0 runs) each player has, aiding in performance evaluations. Your Task ESPN Cricinfo has ODI batting stats for each batsman. The result is paginated across multiple pages. Count the number of ducks in page number 22 . Understanding the Data Source: ESPN Cricinfo's ODI batting statistics are spread across multiple pages, each containing a table of player data. Go to page number 22 . Setting Up Google Sheets: Utilize Google Sheets' IMPORTHTML function to import table data from the URL for page number 22 . Data Extraction and Analysis: Pull the relevant table from the assigned page into Google Sheets. Locate the column that represents the number of ducks for each player. (It is titled \"0\".) Sum the values in the \"0\" column to determine the total number of ducks on that page. Impact By automating the extraction and analysis of cricket batting statistics, CricketPro Insights can: Enhance Analytical Efficiency: Reduce the time and effort required to manually gather and process player performance data. Provide Timely Insights: Deliver up-to-date statistical analyses that aid teams and coaches in making informed decisions. Scalability: Easily handle large volumes of data across multiple pages, ensuring comprehensive coverage of player performances. Data-Driven Strategies: Enable the development of data-driven strategies for player selection, training focus areas, and game planning. Client Satisfaction: Improve service offerings by providing accurate and insightful analytics that meet the specific needs of clients in the cricketing world. What is the total number of ducks across players on page number 22 of ESPN Cricinfo's ODI batting stats ?"
    },
    "q-bbc-weather-api": {
      "answer": "{\n    \"2025-02-08\": \"Sunny and a gentle breeze\",\n    \"2025-02-09\": \"Sunny and a moderate breeze\",\n    \"2025-02-10\": \"Sunny and light winds\",\n    \"2025-02-11\": \"Sunny and light winds\",\n    \"2025-02-12\": \"Light rain showers and a gentle breeze\",\n    \"2025-02-13\": \"Sunny and a gentle breeze\",\n    \"2025-02-14\": \"Sunny and a gentle breeze\",\n    \"2025-02-15\": \"Sunny and a gentle breeze\",\n    \"2025-02-16\": \"Sunny intervals and a gentle breeze\",\n    \"2025-02-17\": \"Sunny and a gentle breeze\",\n    \"2025-02-18\": \"Sunny and a gentle breeze\",\n    \"2025-02-19\": \"Sunny and light winds\",\n    \"2025-02-20\": \"Sunny and light winds\",\n    \"2025-02-21\": \"Sunny and a gentle breeze\"\n}",
      "question": "Weather Data Integration for AgroTech Insights AgroTech Insights is a leading agricultural technology company that provides data-driven solutions to farmers and agribusinesses. By leveraging advanced analytics and real-time data, AgroTech helps optimize crop yields, manage resources efficiently, and mitigate risks associated with adverse weather conditions. Accurate and timely weather forecasts are crucial for making informed decisions in agricultural planning and management. Farmers and agribusinesses rely heavily on precise weather information to plan planting schedules, irrigation, harvesting, and protect crops from extreme weather events. However, accessing and processing weather data from multiple sources can be time-consuming and technically challenging. AgroTech Insights seeks to automate the extraction and transformation of weather data to provide seamless, actionable insights to its clients. AgroTech Insights has partnered with various stakeholders to enhance its weather forecasting capabilities. One of the key requirements is to integrate weather forecast data for specific regions to support crop management strategies. For this purpose, AgroTech utilizes the BBC Weather API , a reliable source of detailed weather information. Your Task As part of this initiative, you are tasked with developing a system that automates the following: API Integration and Data Retrieval: Use the BBC Weather API to fetch the weather forecast for Kuwait City . Send a GET request to the locator service to obtain the city's locationId . Include necessary query parameters such as API key, locale, filters, and search term ( city ). Weather Data Extraction: Retrieve the weather forecast data using the obtained locationId . Send a GET request to the weather broker API endpoint with the locationId . Data Transformation: Extract the localDate and enhancedWeatherDescription from each day's forecast. Iterate through the forecasts array in the API response and map each localDate to its corresponding enhancedWeatherDescription . Create a JSON object where each key is the localDate and the value is the enhancedWeatherDescription . The output would look like this: { \"2025-01-01\" : \"Sunny with scattered clouds\" , \"2025-01-02\" : \"Partly cloudy with a chance of rain\" , \"2025-01-03\" : \"Overcast skies\" , // ... additional days } What is the JSON weather forecast description for Kuwait City ?"
    },
    "q-extract-tables-from-pdf": {
      "answer": "25172",
      "question": "Academic Performance Analysis for EduAnalytics EduAnalytics Corp. is a leading educational technology company that partners with schools and educational institutions to provide data-driven insights into student performance. By leveraging advanced analytics and reporting tools, EduAnalytics helps educators identify trends, improve teaching strategies, and enhance overall student outcomes. One of their key offerings is the Performance Insight Dashboard , which aggregates and analyzes student marks across various subjects and demographic groups. EduAnalytics has recently onboarded Greenwood High School , a large educational institution aiming to optimize its teaching methods and improve student performance in core subjects. Greenwood High School conducts annual assessments in multiple subjects, and the results are compiled into detailed PDF reports each semester. However, manually extracting and analyzing this data is time-consuming and prone to errors, especially given the volume of data and the need for timely insights. To address this, EduAnalytics plans to automate the data extraction and analysis process, enabling Greenwood High School to receive precise and actionable reports without the delays associated with manual processing. As part of this initiative, you are a data analyst at EduAnalytics assigned to develop a module that processes PDF reports containing student marks. Each PDF, named in the format xxx.pdf , includes a comprehensive table listing student performances across various subjects, along with their respective groups. Greenwood High School has specific analytical needs, such as: Subject Performance Analysis: Understanding how students perform in different subjects to identify areas needing improvement. Group-Based Insights: Analyzing performance across different student groups to ensure equitable educational support. Threshold-Based Reporting: Focusing on students who meet or exceed certain performance thresholds to tailor advanced programs or interventions. Your Task This file, q-extract-tables-from-pdf .pdf contains a table of student marks in Maths, Physics, English, Economics, and Biology. Calculate the total English marks of students who scored 48 or more marks in Biology in groups 12 - 36 (including both groups). Data Extraction: : Retrieve the PDF file containing the student marks table and use PDF parsing libraries (e.g., Tabula , Camelot , or PyPDF2 ) to accurately extract the table data into a workable format (e.g., CSV, Excel, or a DataFrame). Data Cleaning and Preparation: Convert marks to numerical data types to facilitate accurate calculations. Data Filtering: Identify students who have scored marks between 48 and Biology in groups 12 - 36 (including both groups). Calculation: Sum the marks of the filtered students to obtain the total marks for this specific cohort. By automating the extraction and analysis of student marks, EduAnalytics empowers Greenwood High School to make informed decisions swiftly. This capability enables the school to: Identify Performance Trends: Quickly spot areas where students excel or need additional support. Allocate Resources Effectively: Direct teaching resources and interventions to groups and subjects that require attention. Enhance Reporting Efficiency: Reduce the time and effort spent on manual data processing, allowing educators to focus more on teaching and student engagement. Support Data-Driven Strategies: Use accurate and timely data to shape educational strategies and improve overall student outcomes. What is the total English marks of students who scored 48 or more marks in Biology in groups 12 - 36 (including both groups)?"
    },
    "q-find-newest-github-user": {
      "answer": "2024-12-04T11:35:10Z",
      "question": "Emerging Developer Talent for CodeConnect CodeConnect is an innovative recruitment platform that specializes in matching high-potential tech talent with forward-thinking companies. As the demand for skilled software developers grows, CodeConnect is committed to staying ahead of trends by leveraging data-driven insights to identify emerging developers—especially those who demonstrate strong community influence on platforms like GitHub. For CodeConnect, a key objective is to tap into regional talent pools to support local hiring initiatives and foster diversity within tech teams. One specific challenge is identifying developers in major tech hubs (such as Shanghai) who not only have established GitHub profiles but also show early signs of influence, as indicated by their follower counts. However, with millions of developers on GitHub and constantly evolving profiles, manually filtering through the data is impractical. CodeConnect needs an automated solution that: Filters Developer Profiles: Retrieves GitHub users based on location and a minimum follower threshold (e.g., over 60 followers) to focus on those with some level of social proof. Identifies the Newest Talent: Determines the most recent GitHub user in the selected group, providing insight into new emerging talent. Standardizes Data: Returns the account creation date in a standardized ISO 8601 format, ensuring consistent reporting across the organization. The recruitment team at CodeConnect is launching a new initiative aimed at hiring young, promising developers in Shanghai—a city known for its vibrant tech community. To support this initiative, the team has commissioned a project to use the GitHub API to find all users located in Shanghai with more than 60 followers. From this filtered list, they need to identify the newest account based on the profile creation date. This information will help the team target outreach efforts to developers who have recently joined the platform and may be eager to explore new career opportunities. Your Task Using the GitHub API , find all users located in the city Chicago with over 80 followers. When was the newest user's GitHub profile created? API Integration and Data Retrieval: Leverage GitHub’s search endpoints to query users by location and filter them by follower count. Data Processing: From the returned list of GitHub users, isolate those profiles that meet the specified criteria. Sort and Format: Identify the \"newest\" user by comparing the created_at dates provided in the user profile data. Format the account creation date in the ISO 8601 standard (e.g., \"2024-01-01T00:00:00Z\"). Impact By automating this data retrieval and filtering process, CodeConnect gains several strategic advantages: Targeted Recruitment: Quickly identify new, promising talent in key regions, allowing for more focused and timely recruitment campaigns. Competitive Intelligence: Stay updated on emerging trends within local developer communities and adjust talent acquisition strategies accordingly. Efficiency: Automating repetitive data collection tasks frees up time for recruiters to focus on engagement and relationship-building. Data-Driven Decisions: Leverage standardized and reliable data to support strategic business decisions in recruitment and market research. Enter the date (ISO 8601, e.g. \"2024-01-01T00:00:00Z\") when the newest user joined GitHub."
    },
    "q-hacker-news-search": {
      "answer": "http://petewarden.typepad.com/searchbrowser/2010/08/paul-grahams-wrong-on-the-value-of-a-hacker-culture.html",
      "question": "Media Intelligence for TechInsight Analytics TechInsight Analytics is a leading market research firm specializing in technology trends and media intelligence. The company provides actionable insights to tech companies, startups, and investors by analyzing online discussions, news articles, and social media posts. One of their key data sources is Hacker News , a popular platform where tech enthusiasts and professionals share and discuss the latest in technology, startups, and innovation. In the rapidly evolving tech landscape, staying updated with the latest trends and public sentiments is crucial for TechInsight Analytics' clients. Manual monitoring of Hacker News posts for specific topics and engagement levels is inefficient and error-prone due to the high volume of daily posts. To address this, TechInsight seeks to automate the process of identifying and extracting relevant Hacker News posts that mention specific technology topics and have garnered significant attention (measured by points). TechInsight Analytics has developed an internal tool that leverages the HNRSS API to fetch the latest Hacker News posts. The tool needs to perform the following tasks: Topic Monitoring: Continuously monitor Hacker News for posts related to specific technology topics, such as \"Artificial Intelligence,\" \"Blockchain,\" or \"Cybersecurity.\" Engagement Filtering: Identify posts that have received a minimum number of points (votes) to ensure the content is highly engaging and relevant. Data Extraction: Extract essential details from the qualifying posts, including the post's link for further analysis and reporting. To achieve this, the team needs to create a program that: Searches Hacker News for the latest posts mentioning a specified topic. Filters these posts based on a minimum points threshold. Retrieves and returns the link to the most relevant post. Your Task Search using the Hacker News RSS API for the latest Hacker News post mentioning Hacker Culture and having a minimum of 82 points. What is the link that it points to? Automate Data Retrieval: Utilize the HNRSS API to fetch the latest Hacker News posts. Use the URL relevant to fetching the latest posts, searching for topics and filtering by a minimum number of points. Extract and Present Data: Extract the most recent <item> from this result. Get the <link> tag inside it. Share the result: Type in just the URL in the answer. What is the link to the latest Hacker News post mentioning Hacker Culture having at least 82 points?"
    },
    "q-nominatim-api": {
      "answer": "30.6501669",
      "question": "Geospatial Data Optimization for UrbanRide UrbanRide is a leading transportation and logistics company operating in major metropolitan areas worldwide. To enhance their service efficiency, optimize route planning, and improve customer satisfaction, UrbanRide relies heavily on accurate geospatial data. Precise bounding box information of cities helps in defining service zones, managing fleet distribution, and analyzing regional demand patterns. As UrbanRide expands into new cities, the company faces the challenge of accurately delineating service areas within these urban environments. Defining the geographical boundaries of a city is crucial for: Route Optimization: Ensuring drivers operate within designated zones to minimize transit times and fuel consumption. Fleet Management: Allocating vehicles effectively across different regions based on demand and service coverage. Market Analysis: Understanding regional demand to tailor services and promotional efforts accordingly. However, manually extracting and verifying bounding box data for each city is time-consuming and prone to inconsistencies, especially when dealing with cities that share names across different countries or have multiple administrative districts. UrbanRide’s data analytics team needs to automate the extraction of precise bounding box coordinates (specifically the minimum and maximum latitude) for various populous cities across different countries. This automation ensures consistency, accuracy, and scalability as the company grows its operations. To achieve this, the team utilizes the Nominatim API , a geocoding service based on OpenStreetMap data, to programmatically retrieve geospatial information. However, challenges arise when cities with the same name exist in multiple countries or have multiple entries within the same country. To address this, the team must implement a method to select the correct city instance based on specific identifiers (e.g., osm_id patterns). Your Task What is the minimum latitude of the bounding box of the city Chongqing in the country China on the Nominatim API? API Integration: Use the Nominatim API to fetch geospatial data for a specified city within a country via a GET request to the Nominatim API with parameters for the city and country. Ensure adherence to Nominatim’s usage policies, including rate limiting and proper attribution. Data Retrieval and Filtering: Parse the JSON response from the API. If multiple results are returned (e.g., multiple cities named “Springfield” in different states), filter the results based on the provided osm_id ending to select the correct city instance. Parameter Extraction: Access the boundingbox attribute. Depending on whether you're looking for the minimum or maximum latitude, extract the corresponding latitude value. Impact By automating the extraction and processing of bounding box data, UrbanRide can: Optimize Routing: Enhance route planning algorithms with precise geographical boundaries, reducing delivery times and operational costs. Improve Fleet Allocation: Allocate vehicles more effectively across defined service zones based on accurate city extents. Enhance Market Analysis: Gain deeper insights into regional performance, enabling targeted marketing and service improvements. Scale Operations: Seamlessly integrate new cities into their service network with minimal manual intervention, ensuring consistent data quality. What is the minimum latitude of the bounding box of the city Chongqing in the country China on the Nominatim API? Value of the minimum latitude"
    },
    "q-pdf-to-markdown": {
      "answer": "Acceptus denego deprimo vobis capillus cupressus vinculum illo.  \nCuratio porro degero tot textus  \n\n[terga laudantium](https://example.com)  \n\nUndique ullus cauda thymum tertius sumptus.  \n\n> Testimonium cupiditas suffoco cornu nostrum.\n\n  - placeat victus vulnus\n\n  - bibo benevolentia vobis\n\n  - tero careo stips carmen deorsum\n\n  - deporto torrens aliquam argentum\n\n  - similique vester adeo\n\n  - trucido compello decet comptus\n\n  - absum verbum temperantia\n\n| vinum    | averto    | terebro bene         |\n|----------|----------|----------------------|\n| decretum | consequuntur | subito solium   |\n| amiculum | tenetur   | terebro vito       |\n\n```\nPraesentium ducimus tego tabula cavus blandior.\nSuscipit volva debeo vulnero tot.\nEveniet absorbeo demoror depulso suffragium totam.\nAmaritudo vereor terreo quisquam totus crur conduco.\nVarietas adipisci quas tersus cavus adhuc conservo bibo vulgivagus.\n\n```\nTabesco nesciunt suus conicio vereor conduco voluntarius aro spero. Subvenio delicate callide universe vespillo utrimque . Vacuus similique approbo acies.\n\nComptus repellendus vere tamen coniuratio contra tempora. Antiquus decretum acceptus. Talio nulla confero amita tabula placeat vomica.\n\n\n-----\n\n",
      "question": "Digital Documentation Transformation for EduDocs Inc. EduDocs Inc. is a leading provider of educational resources and documentation management solutions for academic institutions. With a growing client base comprising universities, colleges, and online learning platforms, EduDocs emphasizes the importance of accessible, well-formatted digital documentation. To maintain high standards and streamline their content delivery, EduDocs continually seeks to enhance its documentation workflows and ensure consistency across all materials. EduDocs manages a vast repository of educational materials, including course syllabi, lecture notes, research papers, and administrative documents. These materials are often provided by clients in various formats, predominantly PDF, which poses challenges for content reuse, editing, and integration into digital platforms. Manually converting PDF documents to Markdown is time-consuming and prone to errors, especially when dealing with large volumes of documents. Additionally, ensuring that the converted Markdown adheres to consistent formatting standards is crucial for maintaining the professional quality of EduDocs' deliverables. To address these challenges, EduDocs aims to automate and standardize the conversion of PDF documents to Markdown format, ensuring that all Markdown files are consistently formatted using Prettier 3.4.2. This initiative will improve efficiency, reduce manual effort, and enhance the overall quality of the documentation provided to clients. Your Task As part of the Documentation Transformation Project , you are a junior developer at EduDocs tasked with developing a streamlined workflow for converting PDF files to Markdown and ensuring their consistent formatting. This project is critical for supporting EduDocs' commitment to delivering high-quality, accessible educational resources to its clients. q-pdf-to-markdown .pdf has the contents of a sample document. Convert the PDF to Markdown: Extract the content from the PDF file. Accurately convert the extracted content into Markdown format, preserving the structure and formatting as much as possible. Format the Markdown: Use Prettier version 3.4.2 to format the converted Markdown file. Submit the Formatted Markdown: Provide the final, formatted Markdown file as your submission. Impact By completing this exercise, you will contribute to EduDocs Inc.'s mission of providing high-quality, accessible educational resources. Automating the PDF to Markdown conversion and ensuring consistent formatting: Enhances Productivity: Reduces the time and effort required to prepare documentation for clients. Improves Quality: Ensures all documents adhere to standardized formatting, enhancing readability and professionalism. Supports Scalability: Enables EduDocs to handle larger volumes of documentation without compromising on quality. Facilitates Integration: Makes it easier to integrate Markdown-formatted documents into various digital platforms and content management systems. What is the markdown content of the PDF, formatted with prettier@3.4.2?"
    },
    "q-scheduled-github-actions": {
      "answer": "https://github.com/mnarayanar/tds-4",
      "question": "Automating Repository Updates for DevSync DevSync Solutions is a mid-sized software development company specializing in collaborative tools for remote teams. With a growing client base and an expanding portfolio of projects, DevSync emphasizes efficient workflow management and robust version control practices to maintain high-quality software delivery. As part of their commitment to maintaining seamless and transparent development processes, DevSync has identified the need to implement automated daily updates to their GitHub repositories. These updates serve multiple purposes: Activity Tracking: Ensuring that each repository reflects daily activity helps in monitoring project progress and team engagement. Automated Documentation: Regular commits can be used to update status files, logs, or documentation without manual intervention. Backup and Recovery: Automated commits provide an additional layer of backup, ensuring that changes are consistently recorded. Compliance and Auditing: Maintaining a clear commit history aids in compliance with industry standards and facilitates auditing processes. Manually managing these daily commits is inefficient and prone to human error, especially as the number of repositories grows. To address this, DevSync seeks to automate the process using GitHub Actions, ensuring consistency, reliability, and scalability across all projects. DevSync's DevOps team has decided to standardize the implementation of GitHub Actions across all company repositories. The objective is to create a scheduled workflow that runs once daily, adds a commit to the repository, and ensures that these actions are consistently tracked and verifiable. As a junior developer or DevOps engineer at DevSync, you are tasked with setting up this automation for a specific repository. This exercise will not only enhance your understanding of GitHub Actions but also contribute to the company's streamlined workflow management. Your Task Create a scheduled GitHub action that runs daily and adds a commit to your repository. The workflow should: Use schedule with cron syntax to run once per day (must use specific hours/minutes, not wildcards) Include a step with your email miryala.narayanareddy@straive.com in its name Create a commit in each run Be located in .github/workflows/ directory After creating the workflow: Trigger the workflow and wait for it to complete Ensure it appears as the most recent action in your repository Verify that it creates a commit during or within 5 minutes of the workflow run Enter your repository URL (format: https://github.com/USER/REPO ):"
    },
    "q-scrape-imdb-movies": {
      "answer": "[\n    {\n        \"id\": \"tt13918776\",\n        \"title\": \"1. The Night Agent\",\n        \"year\": \"2023– \",\n        \"rating\": \"7.5\"\n    },\n    {\n        \"id\": \"tt5040012\",\n        \"title\": \"2. Nosferatu\",\n        \"year\": \"2024\",\n        \"rating\": \"7.4\"\n    },\n    {\n        \"id\": \"tt27444205\",\n        \"title\": \"3. Paradise\",\n        \"year\": \"2025– \",\n        \"rating\": \"7.9\"\n    },\n    {\n        \"id\": \"tt8999762\",\n        \"title\": \"4. The Brutalist\",\n        \"year\": \"2024\",\n        \"rating\": \"8.0\"\n    },\n    {\n        \"id\": \"tt27657135\",\n        \"title\": \"5. Saturday Night\",\n        \"year\": \"2024\",\n        \"rating\": \"7.0\"\n    },\n    {\n        \"id\": \"tt17526714\",\n        \"title\": \"6. The Substance\",\n        \"year\": \"2024\",\n        \"rating\": \"7.3\"\n    },\n    {\n        \"id\": \"tt10919420\",\n        \"title\": \"7. Squid Game\",\n        \"year\": \"2021–2025\",\n        \"rating\": \"8.0\"\n    },\n    {\n        \"id\": \"tt26584495\",\n        \"title\": \"8. Companion\",\n        \"year\": \"2025\",\n        \"rating\": \"7.4\"\n    },\n    {\n        \"id\": \"tt13406094\",\n        \"title\": \"9. The White Lotus\",\n        \"year\": \"2021– \",\n        \"rating\": \"8.0\"\n    },\n    {\n        \"id\": \"tt9218128\",\n        \"title\": \"10. Gladiator II\",\n        \"year\": \"2024\",\n        \"rating\": \"6.6\"\n    },\n    {\n        \"id\": \"tt30057084\",\n        \"title\": \"11. Babygirl\",\n        \"year\": \"2024\",\n        \"rating\": \"6.1\"\n    },\n    {\n        \"id\": \"tt26748649\",\n        \"title\": \"12. High Potential\",\n        \"year\": \"2024– \",\n        \"rating\": \"7.6\"\n    },\n    {\n        \"id\": \"tt28607951\",\n        \"title\": \"13. Anora\",\n        \"year\": \"2024\",\n        \"rating\": \"7.8\"\n    },\n    {\n        \"id\": \"tt14858658\",\n        \"title\": \"14. Blink Twice\",\n        \"year\": \"2024\",\n        \"rating\": \"6.5\"\n    },\n    {\n        \"id\": \"tt16030542\",\n        \"title\": \"15. The Recruit\",\n        \"year\": \"2022– \",\n        \"rating\": \"7.4\"\n    },\n    {\n        \"id\": \"tt7587890\",\n        \"title\": \"16. The Rookie\",\n        \"year\": \"2018– \",\n        \"rating\": \"8.0\"\n    },\n    {\n        \"id\": \"tt11563598\",\n        \"title\": \"17. A Complete Unknown\",\n        \"year\": \"2024\",\n        \"rating\": \"7.7\"\n    },\n    {\n        \"id\": \"tt18259086\",\n        \"title\": \"18. Sonic the Hedgehog 3\",\n        \"year\": \"2024\",\n        \"rating\": \"7.0\"\n    },\n    {\n        \"id\": \"tt20215234\",\n        \"title\": \"19. Conclave\",\n        \"year\": \"2024\",\n        \"rating\": \"7.4\"\n    },\n    {\n        \"id\": \"tt21823606\",\n        \"title\": \"20. A Real Pain\",\n        \"year\": \"2024\",\n        \"rating\": \"7.1\"\n    },\n    {\n        \"id\": \"tt16027074\",\n        \"title\": \"21. Your Friendly Neighborhood Spider-Man\",\n        \"year\": \"2025– \",\n        \"rating\": \"6.3\"\n    },\n    {\n        \"id\": \"tt3288518\",\n        \"title\": \"22. Younger\",\n        \"year\": \"2015–2021\",\n        \"rating\": \"7.8\"\n    },\n    {\n        \"id\": \"tt1262426\",\n        \"title\": \"23. Wicked\",\n        \"year\": \"2024\",\n        \"rating\": \"7.6\"\n    },\n    {\n        \"id\": \"tt31186958\",\n        \"title\": \"24. Prime Target\",\n        \"year\": \"2025– \",\n        \"rating\": \"6.4\"\n    },\n    {\n        \"id\": \"tt8008948\",\n        \"title\": \"25. Den of Thieves: Pantera\",\n        \"year\": \"2025\",\n        \"rating\": \"6.4\"\n    }\n]",
      "question": "Content Curation for StreamFlix Streaming StreamFlix is a rapidly growing streaming service aiming to provide a diverse and high-quality library of movies, TV shows, etc. to its subscribers. To maintain a competitive edge and ensure customer satisfaction, StreamFlix invests heavily in data-driven content curation. By analyzing movie ratings and other key metrics, the company seeks to identify films that align with subscriber preferences and emerging viewing trends. With millions of titles available on platforms like IMDb, manually sifting through titles to select suitable additions to StreamFlix's catalog is both time-consuming and inefficient. To streamline this process, StreamFlix's data analytics team requires an automated solution to extract and analyze movie data based on specific rating criteria. Develop a Python program that interacts with IMDb's dataset to extract detailed information about titles within a specified rating range. The extracted data should include the movie's unique ID, title, release year, and rating. This information will be used to inform content acquisition decisions, ensuring that StreamFlix consistently offers high-quality and well-received films to its audience. Imagine you are a data analyst at StreamFlix, responsible for expanding the platform's movie library. Your task is to identify titles that have received favorable ratings on IMDb, ensuring that the selected titles meet the company's quality standards and resonate with subscribers. To achieve this, you need to: Extract Data: Retrieve movie information from IMDb for all films that have a rating between 6 and 8 . Format Data: Structure the extracted information into a JSON format containing the following fields: id : The unique identifier for the movie on IMDb. title : The official title of the movie. year : The year the movie was released. rating : The IMDb user rating for the movie. Your Task Source: Utilize IMDb's advanced web search at https://www.imdb.com/search/title/ to access movie data. Filter: Filter all titles with a rating between 6 and 8 . Format: For up to the first 25 titles, extract the necessary details: ID, title, year, and rating. The ID of the movie is the part of the URL after tt in the href attribute. For example, tt10078772 . Organize the data into a JSON structure as follows: [ { \"id\" : \"tt1234567\" , \"title\" : \"Movie 1\" , \"year\" : \"2021\" , \"rating\" : \"5.8\" } , { \"id\" : \"tt7654321\" , \"title\" : \"Movie 2\" , \"year\" : \"2019\" , \"rating\" : \"6.2\" } , // ... more titles ] Submit: Submit the JSON data in the text box below. Impact By completing this assignment, you'll simulate a key component of a streaming service's content acquisition strategy. Your work will enable StreamFlix to make informed decisions about which titles to license, ensuring that their catalog remains both diverse and aligned with subscriber preferences. This, in turn, contributes to improved customer satisfaction and retention, driving the company's growth and success in a competitive market. What is the JSON data?"
    },
    "q-wikipedia-outline": {
      "answer": "http://127.0.0.1:8080/",
      "question": "A Country Information API for GlobalEdu GlobalEdu Platforms is a leading provider of educational technology solutions, specializing in creating interactive and informative content for students and educators worldwide. Their suite of products includes digital textbooks, educational apps, and online learning platforms that aim to make learning more engaging and accessible. To enhance their offerings, GlobalEdu Platforms seeks to integrate comprehensive country information into their educational tools, enabling users to access structured and easily navigable content about various nations. With the vast amount of information available on platforms like Wikipedia, manually curating and organizing country-specific data for educational purposes is both time-consuming and inefficient. GlobalEdu Platforms aims to automate this process to ensure that their educational materials are up-to-date, accurate, and well-structured. The key challenges they face include: Content Organization: Presenting information in a structured and hierarchical manner that aligns with educational standards. Scalability: Handling data for a large number of countries without manual intervention. Accessibility: Ensuring that the information is easily accessible from various applications and platforms used by educators and students. Interoperability: Allowing cross-origin requests to integrate the API seamlessly with different front-end applications. To address these challenges, GlobalEdu Platforms has decided to develop a web application that exposes a RESTful API. This API will allow their educational tools to fetch and display structured outlines of Wikipedia pages for any given country. The application needs to: Accept a country name as a query parameter. Fetch the corresponding Wikipedia page for that country. Extract all headings (H1 to H6) from the page. Generate a Markdown-formatted outline that reflects the hierarchical structure of the content. Enable Cross-Origin Resource Sharing (CORS) to allow GET requests from any origin, facilitating seamless integration with various educational platforms. Your Task Write a web application that exposes an API with a single query parameter: ?country= . It should fetch the Wikipedia page of the country, extracts all headings (H1 to H6), and create a Markdown outline for the country. The outline should look like this: ## Contents # Vanuatu ## Etymology ## History ### Prehistory ... API Development: Choose any web framework (e.g., FastAPI) to develop the web application. Create an API endpoint (e.g., /api/outline ) that accepts a country query parameter. Fetching Wikipedia Content: Find out the Wikipedia URL of the country and fetch the page's HTML. Extracting Headings: Use an HTML parsing library (e.g., BeautifulSoup, lxml) to parse the fetched Wikipedia page. Extract all headings (H1 to H6) from the page, maintaining order. Generating Markdown Outline: Convert the extracted headings into a Markdown-formatted outline. Headings should begin with # . Enabling CORS: Configure the web application to include appropriate CORS headers, allowing GET requests from any origin. What is the URL of your API endpoint?"
    }
  }